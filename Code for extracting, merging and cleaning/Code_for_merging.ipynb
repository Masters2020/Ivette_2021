{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Code_for_merging.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQTPORKHbvya","executionInfo":{"status":"ok","timestamp":1619530444167,"user_tz":-120,"elapsed":49272,"user":{"displayName":"Ivette Bonestroo","photoUrl":"","userId":"05386771790343549750"}},"outputId":"353365bc-00fb-4641-c93d-ef3914896a2b"},"source":["#Mount google drive\n","import pickle\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lGnKUZGbbqbs"},"source":["#Adapted from Wegman (2020)\n", "def dataset_shuffler(path, pickle_filepath):\n","        \"\"\"\n","        This function collects our transcript data from Marks set and ours, combines it into one\n","        then stratified-splits it into test_data and training_data.\n","        :param path: input the folder where cleaned transcripts are located\n","        :param pickle_filepath: input the folder path where you want to save the training and test set\n","        :return: returns two variables, one with test, one with training data\n","        also returns two picklefiles in the specified folder containing the same sets.\n","        \"\"\"\n","        import os\n","        import csv\n","        import numpy as np\n","        import pickle\n","        import sys\n","\n","\n","        csv.field_size_limit(sys.maxsize)\n","\n","        alfano = []\n","        os.chdir(path)\n","        with open(path + '/Alfano_Cleaned.csv', 'r', encoding='utf8') as f:\n","            csv_reader = csv.reader(f, delimiter=',')\n","            for i, row in enumerate(csv_reader):\n","                if i == 0:\n","                    continue\n","                else:\n","                    alfano.append(row)\n","\n","        albers = []\n","        os.chdir(path)\n","        with open(path + '/First_Cohort_Cleaned.csv', 'r', encoding='utf8') as f:\n","            csv_reader = csv.reader(f, delimiter=',')\n","            for i, row in enumerate(csv_reader):\n","                if i == 0:\n","                    continue\n","                else:\n","                    albers.append(row)\n","        wegman = []\n","        os.chdir(path)\n","        with open(path + '/Second_Cohort_Cleaned.csv', 'r', encoding='utf8') as f:\n","            csv_reader = csv.reader(f, delimiter=',')\n","            for i, row in enumerate(csv_reader):\n","                if i == 0:\n","                    continue\n","                else:\n","                    wegman.append(row)                 \n","\n","\n","        spring2021 = []\n","        os.chdir(path)\n","        with open(path + '/Spring 2021_Cleaned.csv', 'r', encoding='utf8') as f:\n","            csv_reader = csv.reader(f, delimiter=',')\n","            for i, row in enumerate(csv_reader):\n","                if i == 0:\n","                    continue\n","                else:\n","                    spring2021.append(row)\n","\n","\n","        combined = []\n","\n","        for transcript in alfano:\n","            combined.append([transcript[2], transcript[3]])\n","            \n","        for transcript in albers:\n","            combined.append([transcript[2], transcript[3]])\n","            \n","        for transcript in wegman:\n","            combined.append([transcript[2], transcript[3]])   \n","        for transcript in spring2021:\n","            combined.append([transcript[2], transcript[3]]) \n","        \n","        X = []\n","        y = []\n","        for item in combined:\n","            if item[1] != 'x':\n","                X.append(item[0])\n","            if item[1] != 'x':\n","                y.append(int(item[1]))\n","        X = np.array(X)\n","        y = np.array(y)\n","\n","        from sklearn.model_selection import StratifiedShuffleSplit\n","        indiced = StratifiedShuffleSplit(n_splits=20, test_size=0.2, random_state=0)\n","\n","        for train_index, test_index in indiced.split(X, y):\n","            X_train, X_test = X[train_index], X[test_index]\n","            y_train, y_test = y[train_index], y[test_index]\n","\n","        training_data = []\n","\n","        test_data = []\n","\n","        for i, item in enumerate(X_train):\n","            training_data.append([str(X_train[i]), y_train[i]])\n","        for i, item in enumerate(X_test):\n","            test_data.append([str(X_test[i]), y_test[i]])\n","        import pickle\n","        os.chdir(pickle_filepath)\n","        with open('training_data.pickle', 'wb') as f:\n","            pickle.dump(training_data, f)\n","        with open('test_data.pickle', 'wb') as f:\n","            pickle.dump(test_data, f)\n","        return test_data, training_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-ewmzo9d4ry"},"source":["path = '/content/drive/My Drive/Thesis/'\n","train, test = dataset_shuffler(path, path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aU0mrDXNd8D7"},"source":[""],"execution_count":null,"outputs":[]}]}
