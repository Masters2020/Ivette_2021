{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Notebook_for_extracting_transcripts.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-1oJsj7wunU","outputId":"67007c05-3645-4ee5-ac8b-918241374be1"},"source":["!pip install youtube_transcript_api"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting youtube_transcript_api\n","  Downloading https://files.pythonhosted.org/packages/c9/24/581920922d80f6d6d2feb30787aa4d9e298db83cb081c1de1a63cc7cf121/youtube_transcript_api-0.4.1-py3-none-any.whl\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from youtube_transcript_api) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (3.0.4)\n","Installing collected packages: youtube-transcript-api\n","Successfully installed youtube-transcript-api-0.4.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_aF2Y-6nRbh1","outputId":"ee424f98-3d85-4497-bf61-a9d593954409"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Mon May  4 11:12:28 2020\n","\n","@author: Siebe Albers\n","\"\"\"\n","## edited by Ivette Bonestroo 04-26-2021 \n","\n","#======================================================================== #\n","'load a df with video ids (which will be used for the youtube api to download the transcripts: and later on for extracting the labels  \n","import pandas as pd                        '\n","dataset = pd.read_csv('Youtube Videos New.csv') \n","#with open('IdList_selfWachtedYoutubeVids.txt', encoding=\"utf-8\") as f:\n"," #   idList2 = f.readlines() #txt file with the ids:\n","idList = dataset['ID'].tolist()\n","print(idList)\n","print(len(idList))\n","#alternatively:\n","#dic = dict_oldDf # a dictionary where the keys correspond the the youtubeIDs\n","\n","#======================================================================== #\n","' downloading the transcripts by their ids                           '\n","#======================================================================== #\n","from youtube_transcript_api import YouTubeTranscriptApi\n","import time # just to record how long it takes to download the transcripts\n","STARTTIME = time.time() #plus counting the time,\n","Transcripts_w_timestamps1 =YouTubeTranscriptApi.get_transcripts(video_ids=idList,continue_after_error=True)\n","\n","Transcripts_w_timestamps = Transcripts_w_timestamps1[0]\n","\n","print('time it took:', time.time() - STARTTIME)\n","\n","print( 'len trans', len(Transcripts_w_timestamps)) # see how many could be downloaded\n","\n","\n","# =============================================================================\n","# transcripts that were unable to be extraced:\n","# =============================================================================\n","#ids_thatcouldnotbedownloaded = list( set_originalId - set(downloadedtransIds )\n","#print( 'len downloaded trans:',ids_thatcouldnotbedownloaded)\n","\n","not_extracted = Transcripts_w_timestamps1[1]\n","Transcripts_w_timestamps2 =YouTubeTranscriptApi.get_transcripts(video_ids=not_extracted,continue_after_error=True, cookies = 'cookies.txt') \n","## cookies txt is created based upon the cookies of these videos: \n","##['areIv5h_vss', '4FN12sqoC4Y', 'UX5OwBnhksY', 'JYxJkq5B6ec', 'fMxFSfIV3Dc']\n","## cookies.txt add-on for chrome is used to extract the txt. \n","##The last two rows of the last four generated txtfiles are manually added to the first txtfile: 'cookies.txt'.\n","Transcripts_w_timestamps2 = Transcripts_w_timestamps2[0]\n","print( 'len trans', len(Transcripts_w_timestamps2))\n","\n","# =============================================================================\n","# # creating a dict with transcripts, ψ Writing to string files to (re)create the transcripts\n","# =============================================================================\n","#create a list of video ids, serving as keys for next para\n","IDLIST = list(Transcripts_w_timestamps.keys())\n","IDLIST2 = list(Transcripts_w_timestamps2.keys())\n","\n","trans_dic_fromApi = {}\n","for I in IDLIST:\n","    TRANS = \"\"\n","    trans_dic_fromApi[I] = None\n","    for J in Transcripts_w_timestamps[I]:\n","#        print(J['text'])\n","        TRANS += J['text']\n","        TRANS += \" \"\n","    trans_dic_fromApi[I] = TRANS\n","\n","for I in IDLIST2:\n","    TRANS = \"\"\n","    trans_dic_fromApi[I] = None\n","    for J in Transcripts_w_timestamps2[I]:\n","#        print(J['text'])\n","        TRANS += J['text']\n","        TRANS += \" \"\n","    trans_dic_fromApi[I] = TRANS\n","\n","\n","#======================================================================== #\n","' Exporting to disk         '\n","#======================================================================== #\n","import json\n","with open('ourWatchedYoutubevidsTranscriptsψkeys.json', 'w') as fp:\n","    json.dump(trans_dic_fromApi, fp)\n","\n","\n","\n","\n","\n","## =============================================================================\n","## #creating 1 dictionary for storing all meta Data of the transcripts\n","## =============================================================================\n","#a_meta_dict_transFromApi = { 'transcripts': {I: trans_dic_fromApi[I] for I in trans_dic_fromApi.keys() }, #look at the weird syntac I: ...[i] for i \n","#         'lengths': {I: len(trans_dic_fromApi[I].split()) for I in trans_dic_fromApi.keys() },\n","#         'labels': {I :OldTransDF_idsasKeys['label'][I] for I in trans_dic_fromApi.keys() } #extract labels from old df\n","   #                      }\n","#\n","##check length of the transcript with same Id to see if it complies with lenlist with same id:\n","#len ( a_meta_dict_transFromApi['transcripts']['-5RCmu-HuTg'].split() )\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['_RU4e3KCwQI', 'F70Dm7DWW24', 'zSYp919GLhk', 'h08n4LNzKTU', '_C5zYr4VNsE', 'W1bihTBdEbM', 'L3LINJ3YIE4', 'rUwuOGG1YNU', '_Ylt0dhcN4w', 'jweMwOpX_6I', 'vXRqZoFwKpk', 'k3RR7-64zfM', '4pq5Bq2DdUg', 'areIv5h_vss', '5Jm7bZOFb0g', 'dVxyeJ7pjSo', 'Co66qz-fcSY', 'dCkuTnA8ua4', 'z59UzF5J1Zc', '3OEBFYvDf5s', 'WmCfdMkCO0o', 'mgZahMCulgA', 'eyY1h91OCxo', 'n-UxvT_av0Y', '5oYNbx91oEk', 'yMvVbOQTo94', 'T3zh6erGy6E', 'uu6Mm2rluLc', '1TBxwXYBSCQ', '_GHGFgUB-Ew', 'te8yLsnLkQo', 'p3Y19DTcts4', 'F0g6tg-tPms', '4FN12sqoC4Y', 'xmC7QBswkqU', '27QlHF369ZU', 'UX5OwBnhksY', '78fHed2ciZY', 'rBs4FD4e5WU', '4yXBysQEUyI', '64bqwkNMm_g', 'o9P0eLdlLbg', 'HbfAeLH7UlM', 'yMUZqxz-vYE', 'lbgOs3HngMA', 'zFRkQu4we7I', 'U2rsIqqSOfQ', 'HQirrIiZxF0', 'Sjmi21goWwc', 'JYxJkq5B6ec', 'aYTmUjDJiq0', '-9xP3b41Qik', 'DPRJzNc876A', 'OnOzOZ2KW4s', 'T8-YdgU-CF4', 'gKvWWY03IAw', '7VpWYpAWxrM', 'GIho_gCvYP4', '8absr185b04', '_5MhgeVWmWE', 'BEWz4SXfyCQ', 'F8kH0QXsSgU', 'pzxjkx-szPY', '4UjqFaQq_7I', 'zw7A5hjrz58', 'lgM8SoLzC3U', '-knwgFx7BLQ', 'sVYK9-esCKY', 'QDIfFb5z7eQ', 'u82iSLtylfQ', 'fMxFSfIV3Dc', 'PTz1Dm9iavk', 'cO_fWUiO70s', 'KfV6NPKbLqc', 'UCEjpJG_578', 'ygt2Sq1upcs', 'k-mSuiW0RXs', '-7o3bVInA2A', '6B-_u8a3plE', 'jbQBeHXdWgw', 'f0eIMUHictw', '2zil2GTcPEw', 'k4-rs5JiqqA', '1Jg92tgN_l8', 'X-oJNOFUujo', '9M7oLxDmaek', 'vhYoEAAYi1w', 'K2S75Xrqncw', '1MzxFwglO10', 'EUJozeN7WyM', '5N0SfkKQNhg', 'mAbgrGdx5Xg', 'V6yHPRsxyRo', 'RE9pP1BTBO8', 'DAo_CDPDKA0', 'fSVJffE_Msc', 'ZODE_sRMiT0']\n","97\n","time it took: 38.17760992050171\n","len trans 92\n","len trans 5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X55EZzhMSkVt","outputId":"9303ce1c-048e-4880-cab3-317daaa8eb69"},"source":["print(Transcripts_w_timestamps1[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['areIv5h_vss', '4FN12sqoC4Y', 'UX5OwBnhksY', 'JYxJkq5B6ec', 'fMxFSfIV3Dc']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Tl97FAIsj01P"},"source":["def TranscriptExtractor():\n","    \"This Function extracts a list from the dataset with the following string contents: [Video_ID, Video_Category, Video_Transcript, Video_Rating]\"\n","    import os\n","    import csv\n","    import json\n","    # first we load in all info from the youtube.csv we need\n","   # os.chdir(directory_youtubecsv)\n","    with open('Youtube Videos New.csv', mode='r') as infile:\n","        reader = csv.reader(infile)\n","        youtube_ranked = []\n","        for rows in reader:\n","            youtube_ranked.append(rows)\n","        youtube_ranked_data = {}\n","    for listed in youtube_ranked:\n","        youtube_ranked_data.update({listed[0]: listed[3]})\n","\n","    with open('ourWatchedYoutubevidsTranscriptsψkeys.json') as f:\n","        transcripts = json.load(f)\n","    print(type(youtube_ranked))\n","    id_cat_transcripts = []\n","    id_transcripts = list(transcripts.keys())\n","    for id in id_transcripts:\n","        id_cat_transcripts.append([id, transcripts[id], youtube_ranked_data[id]])\n","\n","    return id_cat_transcripts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVFPU9bCWlxB"},"source":["transcripts_labels = TranscriptExtractor()\n","\n","import csv\n","with open('data_spring2021.csv', 'w', newline='') as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"id\", \"transcript\", \"rating\"])\n","with open(\"data_spring2021.csv\", \"a\", newline=\"\") as f:\n","    writer = csv.writer(f)\n","    writer.writerows(transcripts_labels)"],"execution_count":null,"outputs":[]}]}