{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Job_Manipulate_dataset_WITH_SPRING2021.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru72GpeR3WKY"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmmR0IGU3nvH"
      },
      "source": [
        "import os \n",
        "os.chdir(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph_xkuFg7zZg"
      },
      "source": [
        "import csv, json, sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv8CrnJ971DK"
      },
      "source": [
        "#Code based on Wegman (2020)
        "def TranscriptExtractor():\n",
        "    \n",
        "    \"This Function extracts a list from the dataset with the following string contents: [Video_ID, Video_Category, Video_Transcript, Video_Rating]\"\n",
        "    \"Taken from Job Wegman's (2020) scripts to match the ratings made by Alfano et al with the transcripts\"\n",
        "    \n",
        "    # first we load in all info from the youtube.csv we need\n",
        "    with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset/YouTube.csv\", mode='r') as infile:\n",
        "        reader = csv.reader(infile)\n",
        "        youtube_ranked = []\n",
        "        for rows in reader:\n",
        "            youtube_ranked.append(rows)\n",
        "        youtube_ranked_data = {}\n",
        "\n",
        "    for listed in youtube_ranked:\n",
        "        youtube_ranked_data.update({listed[1][32:]: listed[5]})\n",
        "\n",
        "    ## now for the transcript data\n",
        "    id_cat_transcripts = []\n",
        "\n",
        "    # firearms\n",
        "    with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset/firearms_transcript.json\") as f:\n",
        "        data_transcript_firearms = json.load(f)\n",
        "    # here we fill up a dataset with: [ID, Transcript, Final rating] for firearms\n",
        "    id_transcripts_firearms = list(data_transcript_firearms.keys())\n",
        "    for id in id_transcripts_firearms:\n",
        "        id_cat_transcripts.append([id, data_transcript_firearms[id], youtube_ranked_data[id]])\n",
        "\n",
        "    # fitness\n",
        "    with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset/fitness_transcript.json\") as f:\n",
        "        data_transcript_fitness = json.load(f)\n",
        "    # here we fill up a dataset with: [ID, Transcript, Final rating] for fitness\n",
        "    id_transcripts_fitness = list(data_transcript_fitness.keys())\n",
        "    for id in id_transcripts_fitness:\n",
        "        id_cat_transcripts.append([id, data_transcript_fitness[id], youtube_ranked_data[id]])\n",
        "\n",
        "    # gurus\n",
        "    with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset/gurus_transcript.json\") as f:\n",
        "        data_transcript_gurus = json.load(f)\n",
        "    # here we fill up a dataset with: [ID, Transcript, Final rating] for fitness\n",
        "    id_transcripts_gurus = list(data_transcript_gurus.keys())\n",
        "    for id in id_transcripts_gurus:\n",
        "        id_cat_transcripts.append([id, data_transcript_gurus[id], youtube_ranked_data[id]])\n",
        "\n",
        "    # martial_arts\n",
        "    with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset/martial_arts_transcript.json\") as f:\n",
        "        data_transcript_martial_arts = json.load(f)\n",
        "    # here we fill up a dataset with: [ID, Transcript, Final rating] for fitness\n",
        "    id_transcripts_martial_arts = list(data_transcript_martial_arts.keys())\n",
        "    for id in id_transcripts_martial_arts:\n",
        "        id_cat_transcripts.append([id, data_transcript_martial_arts[id], youtube_ranked_data[id]])\n",
        "\n",
        "    # natural foods\n",
        "    with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset/natural_foods_transcript.json\") as f:\n",
        "        data_transcript_natural_foods = json.load(f)\n",
        "    # here we fill up a dataset with: [ID, Transcript, Final rating] for fitness\n",
        "    id_transcripts_natural_foods = list(data_transcript_natural_foods.keys())\n",
        "    for id in id_transcripts_natural_foods:\n",
        "        id_cat_transcripts.append([id, data_transcript_natural_foods[id], youtube_ranked_data[id]])\n",
        "\n",
        "    # tiny houses\n",
        "    with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset/tiny_houses_transcript.json\") as f:\n",
        "        data_transcript_tiny_houses = json.load(f)\n",
        "    # here we fill up a dataset with: [ID, Transcript, Final rating] for fitness\n",
        "    id_transcripts_tiny_houses = list(data_transcript_tiny_houses.keys())\n",
        "    for id in id_transcripts_tiny_houses:\n",
        "        id_cat_transcripts.append([id, data_transcript_tiny_houses[id], youtube_ranked_data[id]])\n",
        "\n",
        "    ## we now have a full set of 600 entries containing all video ID's, categories, transcripts and rankings\n",
        "    # print(id_cat_transcripts)\n",
        "\n",
        "    # NOW! let's remove all empty transcripts\n",
        "    id_cat_transcripts_emptycleaned = []\n",
        "\n",
        "    for unit in id_cat_transcripts:\n",
        "        if unit[1] == '':\n",
        "            continue\n",
        "\n",
        "        elif unit[1] == \" \":\n",
        "            continue\n",
        "            \n",
        "        else:\n",
        "            id_cat_transcripts_emptycleaned.append(unit)\n",
        "\n",
        "    return id_cat_transcripts_emptycleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ7zpKlEAVvy"
      },
      "source": [
        "alfano_et_al = TranscriptExtractor()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHklZOO5A-AJ"
      },
      "source": [
        "#Sanity check on Alfano et al, as extracted from Job's dataset with Job's code\n",
        "\n",
        "print(len(alfano_et_al))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occ-GXd2NODq"
      },
      "source": [
        "#alfano_et_al[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VUGjFV1m5yV"
      },
      "source": [
        "unq_transcripts = set ()\n",
        "unq_indices = []\n",
        "\n",
        "for i, el in enumerate(alfano_et_al):\n",
        "  \n",
        "  if el[1] in unq_transcripts:\n",
        "    continue\n",
        "\n",
        "  else:\n",
        "    unq_transcripts.add(el[1])\n",
        "    unq_indices.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTQzmlOXncwy"
      },
      "source": [
        "len(unq_transcripts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2QHf-F0pG6h"
      },
      "source": [
        "alfano_unq = []\n",
        "\n",
        "for i, el in enumerate(alfano_et_al):\n",
        "\n",
        "  if i in unq_indices:\n",
        "    alfano_unq.append(el) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02zNbFzwOAvl"
      },
      "source": [
        "len(alfano_unq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYXNraU_OCdz"
      },
      "source": [
        "alfano_unq[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzN1vVHZneH7"
      },
      "source": [
        "def others_extracted():\n",
        "\n",
        "  csv.field_size_limit(sys.maxsize)\n",
        "  \n",
        "  with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset/Transcripts2nddataset.csv\", 'r', encoding='utf8') as f:\n",
        "        \n",
        "        ourwatchedset = list ()\n",
        "\n",
        "        csv_reader = csv.reader(f, delimiter=',')\n",
        "        for i, row in enumerate(csv_reader):\n",
        "          if i == 0:\n",
        "            continue\n",
        "          else:\n",
        "            ourwatchedset.append(row)\n",
        "        \n",
        "        teamA = []\n",
        "        with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset/data_team_a.csv\", 'r', encoding='utf8') as f:\n",
        "            csv_reader = csv.reader(f, delimiter=';')\n",
        "            for i, row in enumerate(csv_reader):\n",
        "                if i == 0:\n",
        "                    continue\n",
        "                else:\n",
        "                    teamA.append(row)            \n",
        "                    \n",
        "        teamB = []\n",
        "        with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/Job's dataset/data_team_b.csv\", 'r', encoding='utf8') as f:\n",
        "            csv_reader = csv.reader(f, delimiter=',')\n",
        "            for i, row in enumerate(csv_reader):\n",
        "                if i == 0:\n",
        "                    continue\n",
        "                else:\n",
        "                    teamB.append(row)\n",
        "\n",
        "        spring2021 = []\n",
        "        with open(\"/content/drive/MyDrive/THESIS/Dataset logistics/data_spring2021.csv\", 'r', encoding='utf8') as f:\n",
        "            csv_reader = csv.reader(f, delimiter=',')\n",
        "            for i, row in enumerate(csv_reader):\n",
        "                if i == 0:\n",
        "                    continue\n",
        "                else:\n",
        "                    spring2021.append(row)\n",
        "\n",
        "  return ourwatchedset, teamA, teamB, spring2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awf6vYCCr0Ju"
      },
      "source": [
        "albers, wegman1, wegman2, spring2021  = others_extracted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCE1KXywr-gr"
      },
      "source": [
        "print(len(albers))\n",
        "print(len(wegman1))\n",
        "print(len(wegman2))\n",
        "print(len(spring2021))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiLPnO-QsQbw"
      },
      "source": [
        "albers[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUo2PZ5ysTqW"
      },
      "source": [
        "wegman1[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHzQi5pDsaTg"
      },
      "source": [
        "wegman2[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eeOvSxJZxj_"
      },
      "source": [
        "spring2021[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvYlN-mmscti"
      },
      "source": [
        "wegman = wegman1 + wegman2\n",
        "print(len(wegman))\n",
        "\n",
        "wegman[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPvLTFwnPd7x"
      },
      "source": [
        "unq_transcripts = set ()\n",
        "unq_indices = []\n",
        "\n",
        "for i, el in enumerate(albers):\n",
        "  \n",
        "  if el[1] in unq_transcripts:\n",
        "    continue\n",
        "\n",
        "  elif el[1] == \" \":\n",
        "    continue\n",
        "\n",
        "  elif el[1] == \"\":\n",
        "    continue\n",
        "  \n",
        "  else:\n",
        "    unq_transcripts.add(el[1])\n",
        "    unq_indices.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNnRZKwOPhtp"
      },
      "source": [
        "albers_unq = []\n",
        "\n",
        "for i, el in enumerate(albers):\n",
        "\n",
        "  if i in unq_indices:\n",
        "    albers_unq.append(el) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waANImPTPeMJ"
      },
      "source": [
        "unq_transcripts = set ()\n",
        "unq_indices = []\n",
        "\n",
        "for i, el in enumerate(wegman):\n",
        "  \n",
        "  if el[1] in unq_transcripts:\n",
        "    continue\n",
        "\n",
        "  elif el[1] == \" \":\n",
        "    continue\n",
        "  \n",
        "  elif el[1] == \"\":\n",
        "    continue\n",
        "    \n",
        "  else:\n",
        "    unq_transcripts.add(el[1])\n",
        "    unq_indices.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1vubWrKPk9a"
      },
      "source": [
        "wegman_unq = []\n",
        "\n",
        "for i, el in enumerate(wegman):\n",
        "\n",
        "  if i in unq_indices:\n",
        "    wegman_unq.append(el) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9RRBl7tZ5rN"
      },
      "source": [
        "unq_transcripts = set ()\n",
        "unq_indices = []\n",
        "\n",
        "for i, el in enumerate(spring2021):\n",
        "  \n",
        "  if el[1] in unq_transcripts:\n",
        "    continue\n",
        "\n",
        "  elif el[1] == \" \":\n",
        "    continue\n",
        "\n",
        "  elif el[1] == \"\":\n",
        "    continue\n",
        "  \n",
        "  else:\n",
        "    unq_transcripts.add(el[1])\n",
        "    unq_indices.append(i)\n",
        "spring2021_unq = []\n",
        "\n",
        "for i, el in enumerate(spring2021):\n",
        "\n",
        "  if i in unq_indices:\n",
        "    spring2021_unq.append(el) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv14DODmQEgU"
      },
      "source": [
        "len(albers_unq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BZLSoK2QGDq"
      },
      "source": [
        "len(wegman_unq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA_tmxsgaPyG"
      },
      "source": [
        "len(spring2021_unq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWg7UcaXQHpn"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "alfano_unq_df = pd.DataFrame.from_records(alfano_unq)\n",
        "albers_unq_df = pd.DataFrame.from_records(albers_unq)\n",
        "wegman_unq_df = pd.DataFrame.from_records(wegman_unq)\n",
        "spring2021_unq_df = pd.DataFrame.from_records(spring2021_unq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe_t-tzxQjsi"
      },
      "source": [
        "alfano_unq_df.to_csv(\"Alfano_Cleaned.csv\")\n",
        "albers_unq_df.to_csv(\"First_Cohort_Cleaned.csv\")\n",
        "wegman_unq_df.to_csv(\"Second_Cohort_Cleaned.csv\")\n",
        "spring2021unq_df.to_csv(\"Spring 2021_Cleaned.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
