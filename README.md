# Thesis_Ivette_2021

Chronological order of steps in this thesis:
1. Extract the transcripts and add the labels
2. Clean each of the datasets
3. Merge the datasets
4. Preprocess each model according to their own preprocessing steps
5. Tune, train and validate the models

GloVe embeddings can be found here: https://nlp.stanford.edu/projects/glove/

All files needed to execute these steps can be found in this github repository.

RoBERT is based on: 
- Pappagari, R., Zelasko, P., Villalba, J., Carmiel, Y., & Dehak, N. (2019).
Hierarchical transformers for long document classification. In 2019
ieee automatic speech recognition and understanding workshop (asru) (pp.
838–844). doi: https://doi.org/10.1109/ASRU46091.2019.9003958

Code for extracting the transcripts and merging them with the other datasets based on:
- Albers, S. (2020). Detecting conspiratorial content using word vector models
trained on reddit conspiracy language for feature enrichment (Unpublished
master’s thesis). Tilburg University, The Netherlands.
- Wegman, J. (2020). A bigram-based approach to conspiracy video classification
(Unpublished master’s thesis). Tilburg University, The Netherlands. 

Code for cleaning based on: 
- Wegman, J. (2020). A bigram-based approach to conspiracy video classification
(Unpublished master’s thesis). Tilburg University, The Netherlands. 

Code for preprocessing of the baseline models based on:
- Ahmed, O. (2020). Classification of conspiratorial contenton youtube – performance
based on different feature extractions (Unpublished master’s
thesis). Tilburg University, The Netherlands.
 
